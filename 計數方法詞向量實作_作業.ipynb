{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 載入所需的Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 載入資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2 Unnamed: 2  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "\n",
       "  Unnamed: 3 Unnamed: 4  \n",
       "0        NaN        NaN  \n",
       "1        NaN        NaN  \n",
       "2        NaN        NaN  \n",
       "3        NaN        NaN  \n",
       "4        NaN        NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read data from spam.csv\n",
    "\n",
    "sms_data = pd.read_csv(\"spam.csv\", encoding = \"ISO-8859-1\", engine='python')\n",
    "sms_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>1</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>0</td>\n",
       "      <td>Will Ì_ b going to esplanade fr home?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>0</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>0</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>0</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2 Unnamed: 2  \\\n",
       "0     0  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1     0                      Ok lar... Joking wif u oni...        NaN   \n",
       "2     1  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3     0  U dun say so early hor... U c already then say...        NaN   \n",
       "4     0  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "...  ..                                                ...        ...   \n",
       "5567  1  This is the 2nd time we have tried 2 contact u...        NaN   \n",
       "5568  0              Will Ì_ b going to esplanade fr home?        NaN   \n",
       "5569  0  Pity, * was in mood for that. So...any other s...        NaN   \n",
       "5570  0  The guy did some bitching but I acted like i'd...        NaN   \n",
       "5571  0                         Rofl. Its true to its name        NaN   \n",
       "\n",
       "     Unnamed: 3 Unnamed: 4  \n",
       "0           NaN        NaN  \n",
       "1           NaN        NaN  \n",
       "2           NaN        NaN  \n",
       "3           NaN        NaN  \n",
       "4           NaN        NaN  \n",
       "...         ...        ...  \n",
       "5567        NaN        NaN  \n",
       "5568        NaN        NaN  \n",
       "5569        NaN        NaN  \n",
       "5570        NaN        NaN  \n",
       "5571        NaN        NaN  \n",
       "\n",
       "[5572 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change label to from string to number\n",
    "# \"ham\" --> 0, \"spam\" --> 1\n",
    "sms_data.loc[sms_data[\"v1\"]==\"ham\", \"v1\"]=0\n",
    "sms_data.loc[sms_data[\"v1\"]==\"spam\", \"v1\"]=1\n",
    "sms_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>747</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  count\n",
       "0      0   4825\n",
       "1      1    747"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check how many spams and hams\n",
    "counted_data = sms_data[[\"v1\", \"v2\"]].groupby(\"v1\").count()\n",
    "counted_data = counted_data.reset_index()\n",
    "counted_data.columns = [\"label\", \"count\"]\n",
    "counted_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 切分資料\n",
    "將資料依據label比例切分為training data與testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         v2  Unnamed: 2  Unnamed: 3  Unnamed: 4\n",
      "label                                          \n",
      "0      4176          38          10           6\n",
      "1       649           4           2           0\n",
      "        v2  Unnamed: 2  Unnamed: 3  Unnamed: 4\n",
      "label                                         \n",
      "0      649           7           0           0\n",
      "1       98           1           0           0\n"
     ]
    }
   ],
   "source": [
    "counted_data[\"ratio\"]=counted_data[\"count\"]/counted_data[\"count\"].sum()\n",
    "counted_data\n",
    "\n",
    "sms_data = sms_data.rename(columns={\"v1\":\"label\"})\n",
    "sms_train = sms_data.sample(frac=counted_data[counted_data[\"label\"]==0][\"ratio\"][0])\n",
    "sms_test = sms_data.drop(sms_train.index)\n",
    "\n",
    "print(sms_train.groupby(\"label\").count())\n",
    "print(sms_test.groupby(\"label\").count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_train.fillna('', inplace=True)\n",
    "sms_test.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 資料預處理\n",
    "* 將所有字詞轉為小寫\n",
    "* 移除所有數字、標點符號"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "      label                                                 v2 Unnamed: 2  \\\n",
      "120     1.0            Ok, be careful ! Don't text and drive !              \n",
      "3489    0.0  I probably won't eat at all today. I think I'm...              \n",
      "3775    0.0                            Did you see that film:)              \n",
      "3722    0.0  I want to sent  &lt;#&gt; mesages today. Thats...              \n",
      "3258    1.0  Designation is software developer and may be s...              \n",
      "\n",
      "     Unnamed: 3 Unnamed: 4  \n",
      "120                         \n",
      "3489                        \n",
      "3775                        \n",
      "3722                        \n",
      "3258                        \n",
      "Test:\n",
      "    label                                                 v2 Unnamed: 2  \\\n",
      "10    0.0          Ela kano.,il download, come wen ur free..              \n",
      "17    0.0  Hi frnd, which is best way to avoid missunders...              \n",
      "29    0.0  Hey I am really horny want to chat or see me n...              \n",
      "36    0.0  Hurt me... Tease me... Make me cry... But in t...              \n",
      "53    0.0                           Bring home some Wendy =D              \n",
      "\n",
      "   Unnamed: 3 Unnamed: 4  \n",
      "10                        \n",
      "17                        \n",
      "29                        \n",
      "36                        \n",
      "53                        \n"
     ]
    }
   ],
   "source": [
    "def preprocess(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        # make all content to lowercase\n",
    "        df.at[i, \"v2\"] = df.iloc[i][\"v2\"].lower()\n",
    "        df.at[i, \"Unnamed: 2\"] = df.iloc[i][\"Unnamed: 2\"].lower()\n",
    "        df.at[i, \"Unnamed: 3\"] = df.iloc[i][\"Unnamed: 3\"].lower()\n",
    "        df.at[i, \"Unnamed: 4\"] = df.iloc[i][\"Unnamed: 4\"].lower()\n",
    "        \n",
    "        # remove all numbers\n",
    "        pattern = r\"\\d+\"\n",
    "        df.at[i, \"v2\"] = re.sub(pattern, \" \", df.iloc[i][\"v2\"])\n",
    "        df.at[i, \"Unnamed: 2\"] = re.sub(pattern, \" \", df.iloc[i][\"Unnamed: 2\"])\n",
    "        df.at[i, \"Unnamed: 3\"] = re.sub(pattern, \" \", df.iloc[i][\"Unnamed: 3\"])\n",
    "        df.at[i, \"Unnamed: 4\"] = re.sub(pattern, \" \", df.iloc[i][\"Unnamed: 4\"])\n",
    "            \n",
    "        # remove all punctuations\n",
    "        punctuation_list = ['.', ',', '!', '?']\n",
    "        for punc in punctuation_list:\n",
    "            df.at[i, \"v2\"] = df.iloc[i][\"v2\"].replace(punc, \" \")\n",
    "            df.at[i, \"Unnamed: 2\"] = df.iloc[i][\"Unnamed: 2\"].replace(punc, \" \")\n",
    "            df.at[i, \"Unnamed: 3\"] = df.iloc[i][\"Unnamed: 3\"].replace(punc, \" \")\n",
    "            df.at[i, \"Unnamed: 4\"] = df.iloc[i][\"Unnamed: 4\"].replace(punc, \" \")\n",
    "    \n",
    "    return df\n",
    "\n",
    "processed_train = preprocess(sms_train)\n",
    "processed_test = preprocess(sms_test)\n",
    "\n",
    "print(\"Train:\")\n",
    "print(processed_train.head())\n",
    "print(\"Test:\")\n",
    "print(processed_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11655"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split data into x_train, y_train, x_test, y_test\n",
    "label_train,v2_train,u2_train,u3_train,u4_train = zip(*processed_train.values)\n",
    "label_test,v2_test,u2_test,u3_test,u4_test = zip(*processed_test.values)\n",
    "# check numbers of unique word in the corpus\n",
    "corpos_train = list(set(\" \".join(list(v2_train + u2_train + u3_train + u4_train)).split()))\n",
    "corpos_test = list(set(\" \".join(list(v2_test + u2_test + u3_test + u4_test)).split()))\n",
    "len(set(corpos_train+corpos_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dino',\n",
       " 'allows',\n",
       " 'bawling',\n",
       " 'Saturday,',\n",
       " 'side',\n",
       " 'wtc',\n",
       " 'finish',\n",
       " 'GOIN',\n",
       " 'u',\n",
       " 'telly',\n",
       " 'mon',\n",
       " 'wish!',\n",
       " 'Lr',\n",
       " 'morphine',\n",
       " 'forgets',\n",
       " 'mental',\n",
       " 'BT-national-rate',\n",
       " 'especially',\n",
       " 'ARE',\n",
       " 'others',\n",
       " 'intelligent,',\n",
       " 'misss',\n",
       " 'Helloooo',\n",
       " 'against',\n",
       " 'keeps',\n",
       " 'Theory:',\n",
       " 'Free-message:',\n",
       " 'match',\n",
       " 'Kay',\n",
       " '@\"',\n",
       " 'Unsubscribe',\n",
       " 'current',\n",
       " 'dismay',\n",
       " 'FRND',\n",
       " 'meso',\n",
       " 'weird,',\n",
       " 'Hol',\n",
       " 'ready,',\n",
       " 'stars',\n",
       " 'battery',\n",
       " 'nuclear',\n",
       " 'goigng',\n",
       " 'words',\n",
       " 'p',\n",
       " 'building',\n",
       " 'shitload',\n",
       " '>>>More',\n",
       " 'half',\n",
       " 'prometazine',\n",
       " 'Hav',\n",
       " 'Gnarls',\n",
       " 'NITE!!',\n",
       " 'wks',\n",
       " 'vomiting',\n",
       " 'bringing',\n",
       " 'simple',\n",
       " 'control',\n",
       " 'generally',\n",
       " 'allowed',\n",
       " 'pimpleseven',\n",
       " 'BBD(pooja)',\n",
       " 'should',\n",
       " 'way!',\n",
       " 'surprise',\n",
       " 'haul',\n",
       " 'anyone',\n",
       " 'Serious',\n",
       " 'nw,',\n",
       " 'nÌÂte?',\n",
       " 'bathe',\n",
       " 'Okey',\n",
       " 'msgs,',\n",
       " 'frndship',\n",
       " 'Ee',\n",
       " '(/M)',\n",
       " 'PROBPOP',\n",
       " 'cut',\n",
       " 'later',\n",
       " 'IMAT',\n",
       " 'msgsometext',\n",
       " 'stock',\n",
       " 'thou',\n",
       " 'yo',\n",
       " 'pissed',\n",
       " 'shuhui',\n",
       " 'Dave',\n",
       " 'treacle?',\n",
       " 'effects',\n",
       " 'mofo',\n",
       " 'philosophy',\n",
       " 'finally',\n",
       " 'Teresa',\n",
       " 'list',\n",
       " 'TRUE:',\n",
       " 'INSTEAD!',\n",
       " 'nichols',\n",
       " 'awaiting',\n",
       " 'Thanx',\n",
       " 'marriage',\n",
       " 'Loving',\n",
       " 'elaya',\n",
       " 'sad',\n",
       " 'cheetos',\n",
       " 'BABES',\n",
       " 'application',\n",
       " 'wrong,',\n",
       " 'honey',\n",
       " 'Gn:-)',\n",
       " 'Of',\n",
       " 'Confidence:',\n",
       " '(come',\n",
       " 'ESPECIALLY',\n",
       " 'inde',\n",
       " 'spoken',\n",
       " 'campus',\n",
       " 'slacking',\n",
       " 'permanent',\n",
       " 'patients',\n",
       " 'colours!',\n",
       " '\\\\\"You\\\\\"',\n",
       " 'Hope',\n",
       " 'if',\n",
       " 'Moral',\n",
       " 'ahead',\n",
       " 'sure',\n",
       " 'you?',\n",
       " 'about',\n",
       " 'when',\n",
       " 'funeral',\n",
       " 'racing',\n",
       " 'info@ringtonekingcouk',\n",
       " 'Activate',\n",
       " 'Vry',\n",
       " 'WJ',\n",
       " 'ths',\n",
       " '/',\n",
       " 'am',\n",
       " 'K,',\n",
       " '-Maruti',\n",
       " 'cuz',\n",
       " 'heap',\n",
       " 'k:)',\n",
       " 'night',\n",
       " 'bed',\n",
       " 'Reading',\n",
       " 'bin',\n",
       " 'drizzling',\n",
       " 'formatting',\n",
       " 'But,',\n",
       " 'As',\n",
       " 'Opt',\n",
       " 'Service',\n",
       " 'lady',\n",
       " 'Wednesday',\n",
       " 'Guys',\n",
       " 'flirt,',\n",
       " 'Sorry',\n",
       " 'brothas?',\n",
       " 'Pls:-)',\n",
       " \"T's&C's\",\n",
       " 'weekly!',\n",
       " \"shouldn't\",\n",
       " \"c's\",\n",
       " 'bus',\n",
       " 'Co',\n",
       " 'POD',\n",
       " 'ACLPM',\n",
       " 'Predicte',\n",
       " 'local',\n",
       " 'easy,',\n",
       " 'bottom',\n",
       " 'TRUE',\n",
       " 'Cashbincouk',\n",
       " 'deep',\n",
       " 'cinema',\n",
       " 'unintentionally',\n",
       " 'did',\n",
       " 'HALF',\n",
       " 'lazy',\n",
       " 'user',\n",
       " 'trained',\n",
       " 'May',\n",
       " 'sliding',\n",
       " 'yesterday,',\n",
       " 'chic',\n",
       " 'puts',\n",
       " 'save',\n",
       " 'again!',\n",
       " 'naked',\n",
       " 'Lucozade',\n",
       " 'cash-balance',\n",
       " 'believe',\n",
       " 'CAMERA,',\n",
       " 'wwwcomuknet',\n",
       " 'Hiya',\n",
       " 'greece',\n",
       " 'ultimatum',\n",
       " 'CustomerCare',\n",
       " 'min',\n",
       " 'izzit',\n",
       " 'activate',\n",
       " 'ENGLAND',\n",
       " 'Afternoon',\n",
       " 'owned',\n",
       " \"How's\",\n",
       " 'univ',\n",
       " 'st?',\n",
       " 'pendent',\n",
       " 'setting',\n",
       " 'WHY',\n",
       " 'burns',\n",
       " 'WORLD',\n",
       " 'disturb',\n",
       " 'Day',\n",
       " 'Ringtone',\n",
       " 'goodnight',\n",
       " 'timing',\n",
       " ':)voice',\n",
       " 'comedycant',\n",
       " 'pressure',\n",
       " 'wildest',\n",
       " 'xt',\n",
       " 'C-IN',\n",
       " 'only',\n",
       " 'Havnt',\n",
       " \"b'day\",\n",
       " 'you',\n",
       " 'angry',\n",
       " 'Until',\n",
       " 'knickers',\n",
       " 'less',\n",
       " 'REMEMBER',\n",
       " \"WOULDN'T\",\n",
       " 'KING\\\\\"!',\n",
       " 'purchases',\n",
       " 'home?',\n",
       " 'him?',\n",
       " 'baby!',\n",
       " 'Lmao',\n",
       " 'temp',\n",
       " 'expensive',\n",
       " 'remains',\n",
       " 'financial',\n",
       " 'Congratulations',\n",
       " 'Starts',\n",
       " 'Titles:',\n",
       " 'hrs',\n",
       " 'monkeys',\n",
       " 'ppmPOBoxBhamBXE',\n",
       " 'her',\n",
       " 'Drop',\n",
       " 'July',\n",
       " 'Excellent',\n",
       " 'stuck',\n",
       " 'ASAP',\n",
       " 'WRC',\n",
       " 'rd',\n",
       " 'sure,',\n",
       " 'POBox,',\n",
       " 'more',\n",
       " 'xafter',\n",
       " 'include:',\n",
       " 'ass!!',\n",
       " 'peace',\n",
       " 'maxå£',\n",
       " 'Sender:Name',\n",
       " 'rude',\n",
       " 'President',\n",
       " '//',\n",
       " 'donåÕt',\n",
       " 'tests',\n",
       " 'Jst',\n",
       " 'rd',\n",
       " 'thia',\n",
       " 'like',\n",
       " 'morrow',\n",
       " 'lucky;)',\n",
       " 'that!',\n",
       " 'EACHOTHER',\n",
       " 'Joke',\n",
       " 'coming',\n",
       " 'Em,',\n",
       " '&gt;:(',\n",
       " 'Thanks',\n",
       " 'xp/wk',\n",
       " 'Coz',\n",
       " 'ABOUTAS',\n",
       " 'first',\n",
       " 'group',\n",
       " 'leastWhich',\n",
       " 'Omg',\n",
       " 'victoria',\n",
       " 'incident',\n",
       " ',',\n",
       " '^',\n",
       " 'years!!',\n",
       " 'oredi',\n",
       " 'mayb?',\n",
       " 'prob',\n",
       " 'whole',\n",
       " 'aeroplane',\n",
       " 'names',\n",
       " 'Winaweek,',\n",
       " 'showing',\n",
       " '*covers',\n",
       " 'activities',\n",
       " 'distance',\n",
       " 'urgnt,',\n",
       " 'whether',\n",
       " 'Lands',\n",
       " 'picked',\n",
       " 'p',\n",
       " 'cool,',\n",
       " \"weather's\",\n",
       " 'Search',\n",
       " 'å£',\n",
       " \"RCT'\",\n",
       " 'TXTAUCTION!Txt',\n",
       " 'luvs',\n",
       " 'd',\n",
       " 'p/msg',\n",
       " 'bitch',\n",
       " 'MISSED',\n",
       " 'Urgent',\n",
       " 'places',\n",
       " 'pendent?',\n",
       " 'okay',\n",
       " 'WK',\n",
       " 'dad',\n",
       " 'thk',\n",
       " 'castor',\n",
       " 'M',\n",
       " 'Hm',\n",
       " 'hw',\n",
       " 'knackered',\n",
       " 't&csBCMWCNXXcallcostppmmobilesvary',\n",
       " 'fixd',\n",
       " 'penis',\n",
       " 'http://tms',\n",
       " 'lehHaha',\n",
       " 'cliffs',\n",
       " 'itself',\n",
       " 'Day\\\\\"',\n",
       " 'support',\n",
       " 'secondary',\n",
       " 'mca',\n",
       " 'envelope,',\n",
       " 'bulbs',\n",
       " ';V',\n",
       " 'i\\x89Ûªm',\n",
       " 'Other',\n",
       " 'HON',\n",
       " 'Adrian',\n",
       " 'BOUT',\n",
       " '<fone',\n",
       " '(',\n",
       " 'means;',\n",
       " '(Ranju)',\n",
       " 'mins',\n",
       " 'IBHltd',\n",
       " 'bird!',\n",
       " 'Babe!',\n",
       " 'plans',\n",
       " 'messagepandy',\n",
       " 'marrow',\n",
       " 'summon',\n",
       " 'Senthil',\n",
       " 'cartons',\n",
       " 'Help',\n",
       " 'GIRL',\n",
       " 'Pain',\n",
       " 'gd&the',\n",
       " 'South',\n",
       " 'requirements',\n",
       " 'dude!',\n",
       " 'HOT',\n",
       " ')McFly-All',\n",
       " 'WRKIN?',\n",
       " 'musical',\n",
       " 'singapore',\n",
       " 'quarter,',\n",
       " 'reason',\n",
       " 'yours,',\n",
       " 'loneliness',\n",
       " '-Bajarangabali',\n",
       " 'AV',\n",
       " 'SOMETHIN',\n",
       " 'INDYAROCKSCOM',\n",
       " 'NOW!',\n",
       " 'll',\n",
       " 'kid',\n",
       " 'ISVIMPORTANT',\n",
       " 'Uks',\n",
       " 'thatnow',\n",
       " ':-(',\n",
       " 'price',\n",
       " 'Australia',\n",
       " 'POLYC',\n",
       " 'question',\n",
       " 'Show',\n",
       " 'U\\x89Ûªve',\n",
       " \"Can't\",\n",
       " 'Neighbour:',\n",
       " 'ryan',\n",
       " 'smaller',\n",
       " 'sent',\n",
       " 'Natalja',\n",
       " 'pizza',\n",
       " 'netflix',\n",
       " 'Thank',\n",
       " 'girls',\n",
       " 'spoiled?',\n",
       " \"B'tooth*\",\n",
       " 'Nokia',\n",
       " 'Yellow',\n",
       " 'trash',\n",
       " 'WIN',\n",
       " 'washob',\n",
       " 'kaypoh',\n",
       " 'aretaking',\n",
       " 'chennai:)because',\n",
       " 'class:-)',\n",
       " 'Nikiyunet',\n",
       " 'joking,',\n",
       " 'saved',\n",
       " 'Olol',\n",
       " 'skyving',\n",
       " 'Ptbo',\n",
       " 'Woodland',\n",
       " 'powerful',\n",
       " 'u,',\n",
       " 'babies',\n",
       " 'Natalie',\n",
       " 'Nobut',\n",
       " \"who's\",\n",
       " 'bsn',\n",
       " '(an',\n",
       " \"I'd\",\n",
       " 'Election',\n",
       " 'msging',\n",
       " 'there',\n",
       " 'me!;',\n",
       " '//',\n",
       " 'High',\n",
       " 'OH',\n",
       " 'Sender:',\n",
       " 'ar',\n",
       " 'Here',\n",
       " '-call',\n",
       " 'I-ntimate',\n",
       " 'Halloween',\n",
       " 'no',\n",
       " 'suite',\n",
       " 'promises',\n",
       " 'xy',\n",
       " ':/',\n",
       " 'hill',\n",
       " 'thanks',\n",
       " 'pple',\n",
       " ':)why',\n",
       " 'u',\n",
       " 'Sh!jas',\n",
       " 'yes',\n",
       " 'try',\n",
       " 'winner!',\n",
       " 'flat,',\n",
       " 'cars',\n",
       " 'explicit',\n",
       " 'Year,',\n",
       " 'workout',\n",
       " 'Fantasy',\n",
       " 'Messages',\n",
       " 'game',\n",
       " 'Idk',\n",
       " 'Kidz,',\n",
       " 'vava',\n",
       " 'T&Cs/stop',\n",
       " 'pleasured',\n",
       " 'wat',\n",
       " 'BIG',\n",
       " 'watchin',\n",
       " 'blogging',\n",
       " 'unless',\n",
       " 'four',\n",
       " 'later',\n",
       " 'Stupid',\n",
       " 'care!',\n",
       " 'empty',\n",
       " \"week's\",\n",
       " 'day?',\n",
       " 'labor',\n",
       " 'IT(NOW',\n",
       " 'mobsicom',\n",
       " 'Okie',\n",
       " '[in',\n",
       " 'evry',\n",
       " 'loving',\n",
       " 'slowly',\n",
       " 'nearby',\n",
       " 'decades!',\n",
       " 'heart',\n",
       " 'barring',\n",
       " 'texted',\n",
       " 'nobbing',\n",
       " 'parish',\n",
       " 'right',\n",
       " 'WU',\n",
       " 'THATåÕSCOOL',\n",
       " 'country',\n",
       " 'that',\n",
       " 'Mathews',\n",
       " 'Ì_',\n",
       " 'christ',\n",
       " 'slow',\n",
       " 'todays',\n",
       " 'appt',\n",
       " 'paragon',\n",
       " 'second,',\n",
       " 'tm',\n",
       " 'This',\n",
       " 'AWARD!',\n",
       " 'dialogue',\n",
       " 'wwwtxttowincouk',\n",
       " 'loosing',\n",
       " 'scores',\n",
       " 'first',\n",
       " 'it,U',\n",
       " 'Now!T&',\n",
       " 'cold',\n",
       " 'wait?',\n",
       " 'ate',\n",
       " 'energy\\\\\"',\n",
       " 'SAE',\n",
       " 'raglan',\n",
       " 'msg',\n",
       " 'trip',\n",
       " 'Sara',\n",
       " 'play',\n",
       " '(Send',\n",
       " 'leh,',\n",
       " 'lorWe',\n",
       " 'SOZ',\n",
       " 'not!',\n",
       " 'Ur',\n",
       " 'yeah',\n",
       " 'DeliveredTomorrow',\n",
       " 'Hey',\n",
       " 'X',\n",
       " 'WARD',\n",
       " 'head\\x89Û_',\n",
       " 'Jackpot!',\n",
       " 'seing',\n",
       " 'auto',\n",
       " 'IM',\n",
       " 'Bored',\n",
       " 'traffic',\n",
       " 'id=baecefff*&first=true:-JUL-',\n",
       " 'pride',\n",
       " 'add',\n",
       " 'WAITING',\n",
       " 'sef',\n",
       " 'senthil',\n",
       " 'behalf',\n",
       " 'mre',\n",
       " 'gal',\n",
       " 'jay',\n",
       " 'impressively,',\n",
       " 'her:',\n",
       " 'won!',\n",
       " 'fgkslpoPW',\n",
       " 'boring',\n",
       " 'morning',\n",
       " 'jump',\n",
       " 'seven',\n",
       " 'know?',\n",
       " 'xxx',\n",
       " 'awake',\n",
       " 'legal',\n",
       " 'Awarded',\n",
       " 'u:',\n",
       " 'contract',\n",
       " 'Reply',\n",
       " 'nearly',\n",
       " 'easter',\n",
       " 'cancel',\n",
       " 'misfits',\n",
       " 'screaming',\n",
       " 'Queen',\n",
       " 'Sez,',\n",
       " 'FIELDOF',\n",
       " 'AUGUST!',\n",
       " 'Yay',\n",
       " 'dhorte',\n",
       " 'sun',\n",
       " \"tm'ing\",\n",
       " 'is',\n",
       " 'Identifier',\n",
       " 'whoever',\n",
       " 'Trackmarque',\n",
       " 'å£',\n",
       " 'pilates',\n",
       " '--',\n",
       " 'check',\n",
       " 'wallpaper-all',\n",
       " 'denis',\n",
       " 'sandiago',\n",
       " 'changes',\n",
       " 'chances',\n",
       " 'once',\n",
       " 'mega',\n",
       " 'njan',\n",
       " \"mum's\",\n",
       " \"Dip's\",\n",
       " 'masters',\n",
       " 'wml?id=ada&first=trueåÁC',\n",
       " 'Hot',\n",
       " '+',\n",
       " 'coincidence?',\n",
       " 'people!\\\\\"',\n",
       " ':-D',\n",
       " 'leh?',\n",
       " 'Nature',\n",
       " 'system',\n",
       " 'SIX',\n",
       " '(flights',\n",
       " 'g',\n",
       " 'murdered',\n",
       " 'unsubscribed',\n",
       " 'bedrm-$',\n",
       " ':)',\n",
       " 'mjzgroup',\n",
       " 'urgent',\n",
       " 'Deliver',\n",
       " 'nyt',\n",
       " 'father:',\n",
       " 'worth',\n",
       " 'forgotten',\n",
       " 'cancer',\n",
       " '\\\\Wen',\n",
       " 'Nope',\n",
       " 'Single',\n",
       " 'chatting',\n",
       " 'Daddy,',\n",
       " 'rem',\n",
       " 'beggar',\n",
       " 'details',\n",
       " 'stone,',\n",
       " 'it',\n",
       " 'Easter',\n",
       " '*I*',\n",
       " 'embarassed,',\n",
       " 'Normally',\n",
       " 'xx',\n",
       " 'small',\n",
       " ':)going',\n",
       " 'Aight,',\n",
       " 'in:',\n",
       " 'THECD',\n",
       " 'thanx!',\n",
       " 'don\\x89Û÷t',\n",
       " 'How',\n",
       " 'loved',\n",
       " 'valentine',\n",
       " 'into:',\n",
       " 'hustle',\n",
       " 'does',\n",
       " 'fish',\n",
       " '\\\\Its',\n",
       " 'browser',\n",
       " 'anybody',\n",
       " 'uncle',\n",
       " 'thinking',\n",
       " 'BIRTHDAY',\n",
       " 'BEER',\n",
       " 'Txt:',\n",
       " 'sane',\n",
       " 'meeting',\n",
       " \"nobody's\",\n",
       " 'Loves',\n",
       " 'xmas',\n",
       " 'bedroom',\n",
       " 'died',\n",
       " 'amt',\n",
       " 'åÒHarry',\n",
       " 'singing',\n",
       " 'Water',\n",
       " 'left',\n",
       " 'disagreeable',\n",
       " 'WRG',\n",
       " 'there!',\n",
       " 'be',\n",
       " 'play,',\n",
       " 'didn\\x89Û÷t',\n",
       " 'å£',\n",
       " 'å£,',\n",
       " 'figures,',\n",
       " 'away!!',\n",
       " 'bother',\n",
       " 'award',\n",
       " 'goin',\n",
       " 'Left',\n",
       " 'sugar',\n",
       " 'UAWAKE',\n",
       " 'TsCs',\n",
       " 'assume',\n",
       " 'minimum',\n",
       " 'pehle',\n",
       " 'virgin',\n",
       " 'mins',\n",
       " 'prolly',\n",
       " 'panalambut',\n",
       " 'DOESNT',\n",
       " 'sha',\n",
       " 'half-th',\n",
       " 'persondie',\n",
       " 'magazine,',\n",
       " 'Street',\n",
       " 'TsandCs',\n",
       " 'wins',\n",
       " 'cuddled',\n",
       " 'åÐ',\n",
       " 'watching',\n",
       " 'LET',\n",
       " 'doing',\n",
       " 'bt',\n",
       " 'life',\n",
       " 'p',\n",
       " 'FIRST',\n",
       " 'today',\n",
       " 'ever',\n",
       " 'Tyler',\n",
       " 'Mobileupd',\n",
       " 'Always',\n",
       " 'cheaper',\n",
       " 'FreeMsg>FAV',\n",
       " 'Hi',\n",
       " 'Melle',\n",
       " 'MEGA',\n",
       " 'return',\n",
       " 'lor,',\n",
       " 'Woods',\n",
       " 'rock',\n",
       " 'thNOVBEHIND',\n",
       " 'joking',\n",
       " 'webadres',\n",
       " 'i',\n",
       " 'Uh,',\n",
       " 'sweater',\n",
       " 'pierre',\n",
       " 'reached',\n",
       " \"hasn't\",\n",
       " '*',\n",
       " 'Goodmorning,my',\n",
       " 'Yeah',\n",
       " '\\\\Our',\n",
       " 'Hack',\n",
       " 'POST,',\n",
       " 'AVE',\n",
       " 'shipping',\n",
       " 'ovulatewhen',\n",
       " 'COLLECT',\n",
       " \"'Need'\",\n",
       " 'Jolly',\n",
       " 'oh',\n",
       " 'HL',\n",
       " 'Red',\n",
       " 'key',\n",
       " 'kintu',\n",
       " 'Madam,regret',\n",
       " 'Huh',\n",
       " 'kalisidare',\n",
       " 'cream',\n",
       " 'pa',\n",
       " 'winning',\n",
       " 'normally',\n",
       " 'badass',\n",
       " 'Yes',\n",
       " 'mah',\n",
       " 'engin',\n",
       " 'w',\n",
       " 'week,',\n",
       " 'Us',\n",
       " 'fat!',\n",
       " 'dump',\n",
       " 'sound',\n",
       " 'Okie',\n",
       " 'team',\n",
       " 'mail',\n",
       " 'sale',\n",
       " 'throw',\n",
       " 'madstini',\n",
       " \"didn't\",\n",
       " 'her',\n",
       " 'wwwSMSac/u/nataliek',\n",
       " 'Sullivan',\n",
       " 'Argh',\n",
       " 'catching',\n",
       " 'Haha',\n",
       " 'Chat',\n",
       " 'jam,',\n",
       " 'cal',\n",
       " 'subletting',\n",
       " 'term',\n",
       " 'SHE',\n",
       " 'numbers',\n",
       " 'thru',\n",
       " 'strong:)',\n",
       " 'color',\n",
       " 'relation',\n",
       " 'Reason,',\n",
       " 'Opinion',\n",
       " 'Thurs,',\n",
       " 'kissing!',\n",
       " 'year',\n",
       " 'and,',\n",
       " 'askd',\n",
       " 'recorder',\n",
       " 'paper,',\n",
       " 'saved!',\n",
       " 'difference',\n",
       " 'office',\n",
       " 'better',\n",
       " 'idu',\n",
       " 'shove',\n",
       " 'splash',\n",
       " 'watever',\n",
       " \"it's\",\n",
       " 'dropped',\n",
       " 'yours!',\n",
       " 'PEACH!',\n",
       " \"priscilla's\",\n",
       " 'hopeing',\n",
       " 'messageno',\n",
       " \"me!''\",\n",
       " 'TWILIGHT',\n",
       " 'bong',\n",
       " 'worried',\n",
       " 'DPS,',\n",
       " 'START',\n",
       " \"they'll\",\n",
       " 'So,',\n",
       " 'å£',\n",
       " 'OKDEN',\n",
       " 'guy',\n",
       " 'resubmit',\n",
       " 'Rental',\n",
       " 'start',\n",
       " 'neekunna',\n",
       " 'conform',\n",
       " \"That's\",\n",
       " 'correctly',\n",
       " 'weeks!',\n",
       " 'Second',\n",
       " 'HOME',\n",
       " 'nelson',\n",
       " 'usGET',\n",
       " 'error',\n",
       " 'shud',\n",
       " 'gaytextbuddycom',\n",
       " 'bud',\n",
       " 'try,',\n",
       " 'considering',\n",
       " 'fixes',\n",
       " 'olave',\n",
       " 'Nah,',\n",
       " 'prakasamanu',\n",
       " 'FFFFUUUUUUU',\n",
       " 'dinner?',\n",
       " 'virtual',\n",
       " 'fyi',\n",
       " 'syrup',\n",
       " 'machines',\n",
       " 'gently',\n",
       " 'Emotion',\n",
       " 'standing|',\n",
       " 'melt',\n",
       " 'place',\n",
       " 'PO',\n",
       " 'god',\n",
       " 'getting',\n",
       " 'envelope',\n",
       " 'outreach',\n",
       " 'visionsmscom',\n",
       " 'home',\n",
       " 'Hmmmm?',\n",
       " 'hole,',\n",
       " 'he!',\n",
       " 'straight',\n",
       " 'cheese',\n",
       " 'Year!',\n",
       " 'THE',\n",
       " 'normp/tone',\n",
       " 'fone',\n",
       " 'SI',\n",
       " 'PRICE',\n",
       " 'obviously,',\n",
       " 'uscedu',\n",
       " 'station',\n",
       " 'wwwdbuknet',\n",
       " 'Forgot',\n",
       " 'contents',\n",
       " 'lyricalladie(/F)',\n",
       " 'weather',\n",
       " 'heartgn',\n",
       " 'å£',\n",
       " 'breezy',\n",
       " 'Mths',\n",
       " 'Eve',\n",
       " 'offer',\n",
       " 'Golf',\n",
       " 'edwards',\n",
       " 'Genius',\n",
       " 'china',\n",
       " 'finished',\n",
       " 'mls',\n",
       " 'sec',\n",
       " 'notixiquating',\n",
       " 'full',\n",
       " 'spoke',\n",
       " 'colleg',\n",
       " 'old',\n",
       " 'amt',\n",
       " 'Oz?',\n",
       " 'not',\n",
       " 'Mm',\n",
       " 'aquarius',\n",
       " 'ILLSPEAK',\n",
       " 'online',\n",
       " 'Baby',\n",
       " 'cover',\n",
       " 'HELLOGORGEOUS,',\n",
       " 'me',\n",
       " 'eveB-)',\n",
       " 'No,',\n",
       " 'Pleasure',\n",
       " 'sorrow,',\n",
       " 'stayed',\n",
       " 'words',\n",
       " 'long,',\n",
       " 'hurried',\n",
       " 'frequently',\n",
       " 'covers',\n",
       " 'S:)',\n",
       " ':-)keep',\n",
       " 'pick',\n",
       " 'Foned',\n",
       " 'deals',\n",
       " 'first,',\n",
       " ';)',\n",
       " 'settled',\n",
       " 'visa',\n",
       " 'Sun',\n",
       " 'nuther',\n",
       " 'wonderful',\n",
       " 'bunch',\n",
       " 'shindig',\n",
       " 'bday',\n",
       " 'dismissial',\n",
       " 'uSo',\n",
       " 'download',\n",
       " 'Ball',\n",
       " 'ding',\n",
       " 'School',\n",
       " 'screwd',\n",
       " 'TONES!Reply',\n",
       " 'Make',\n",
       " 'allday!',\n",
       " 'moms',\n",
       " 'ends',\n",
       " 'IåÕL',\n",
       " 'me,',\n",
       " 'MOBILE',\n",
       " 'casting',\n",
       " 'archive',\n",
       " 'now!\"',\n",
       " 'de:-)',\n",
       " \"you'll\",\n",
       " 'Try',\n",
       " 'REAL',\n",
       " 'nervous',\n",
       " 'Being',\n",
       " 'never',\n",
       " 'out',\n",
       " 'needs',\n",
       " 'Dey',\n",
       " 'desert',\n",
       " 'vodafone',\n",
       " 'ringtone-get',\n",
       " '//!',\n",
       " 'PICSFREE',\n",
       " 'box',\n",
       " 'Same,',\n",
       " 'ten',\n",
       " 'yr',\n",
       " 'Points',\n",
       " 'lor',\n",
       " 'UNBELIEVABLE',\n",
       " 'good!',\n",
       " 'maga',\n",
       " 'Luv',\n",
       " ...]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctuation_list = ['.', ',', '!', '?']\n",
    "pattern = r\"\\d+\"\n",
    "new_corpos_train = []\n",
    "for punc in punctuation_list:\n",
    "    for word in corpos_train:\n",
    "        tmp_w = re.sub(pattern, \"\", word.replace(punc, \"\")).strip()\n",
    "        if tmp_w:\n",
    "            new_corpos_train.append(tmp_w)\n",
    "corpos_train = new_corpos_train\n",
    "corpos_train\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "可以發現種共有7708個字詞，這裡使用TF-IDF將來選取最高分的前2000個字詞\n",
    "(若忘記的學員可參考先前TF-IDF課程章節或[此篇教學](https://ithelp.ithome.com.tw/articles/10228815?sc=iThelpR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique word of tfidf_train: 6828\n",
      "['dabbles' 'ûò' 'frnds' ... 'bored' 'borin' 'boring']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "\n",
    "n = 2000\n",
    "\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=False, stop_words=None,  smooth_idf=True, norm='l2')\n",
    "\n",
    "tfidf = vectorizer.fit_transform(corpos_train+corpos_test)\n",
    "print(f\"Number of unique word of tfidf_train: {len(vectorizer.vocabulary_)}\")\n",
    "tfidf_sorting = np.argsort(tfidf.toarray()).flatten()[::-1]\n",
    "top_n = np.array(vectorizer.get_feature_names())[tfidf_sorting][:n]\n",
    "print(top_n)\n",
    "\n",
    "# tfidf_train = vectorizer.fit_transform(corpos_train)\n",
    "# print(f\"Number of unique word of tfidf_train: {len(vectorizer.vocabulary_)}\")\n",
    "# tfidf_train_sorting = np.argsort(tfidf_train.toarray()).flatten()[::-1]\n",
    "# top_n_train = np.array(vectorizer.get_feature_names())[tfidf_train_sorting][:n]\n",
    "# print(top_n_train)\n",
    "\n",
    "# tfidf_test = vectorizer.fit_transform(corpos_test)\n",
    "# print(f\"Number of unique word of tfidf_test: {len(vectorizer.vocabulary_)}\")\n",
    "# tfidf_test_sorting = np.argsort(tfidf_test.toarray()).flatten()[::-1]\n",
    "# top_n_test = np.array(vectorizer.get_feature_names())[tfidf_test_sorting][:n]\n",
    "# print(top_n_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立共現矩陣"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "create_co_matrix() got multiple values for argument 'window_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-96815543ea63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m co_matrix = create_co_matrix(corpos_train, vectorizer.get_feature_names(), vectorizer.vocabulary_,\n\u001b[0;32m---> 68\u001b[0;31m                             window_size=3)\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0mco_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: create_co_matrix() got multiple values for argument 'window_size'"
     ]
    }
   ],
   "source": [
    "def create_co_matrix(corpus: List[str], vocab_list: List[str], word2idx: dict,\n",
    "                     window_size: int=1, use_weighting: bool=False, verbose: bool=False) -> np.ndarray:\n",
    "    '''Function to create co-occurrence matrix\n",
    "    '''\n",
    "    #initialize co-occurrence matrix\n",
    "    co_matrix = np.zeros(shape=(len(vocab_list), len(vocab_list)), dtype=np.int32)\n",
    "    sms_ids = []\n",
    "    for idx, sms in enumerate(corpus):\n",
    "        if sms and sms in word2idx:\n",
    "#             print(f\"idx:{idx}, sms:{sms}\")\n",
    "            sms_ids.insert(idx, word2idx[sms])\n",
    "        \n",
    "\n",
    "    for idx, sms in enumerate(corpus):\n",
    "        for center_i, center_word_id in enumerate(sms_ids):\n",
    "            if center_word_id:\n",
    "\n",
    "                left_idx = idx - window_size if idx - window_size >= 0 else 0\n",
    "                context_ids = sms[left_idx:idx]\n",
    "                print(f\"center_word_id: {center_word_id}, left_id:{left_id}\")\n",
    "                for left_i, left_id in enumerate(context_ids):\n",
    "                    co_matrix[center_word_id, left_id] += 1\n",
    "                    co_matrix[left_id, center_word_id] += 1\n",
    "        \n",
    "        if verbose:\n",
    "            if idx != 0 and idx%500 == 0:\n",
    "                    print(f\"finishing {idx+1}/{len(corpus)}\")\n",
    "    print(\"Done\")\n",
    "#     if use_weighting:\n",
    "#         if use weighting, then we set the co-occurrence with the word itself to 1.0\n",
    "        ##<your code>###\n",
    "\n",
    "    return co_matrix\n",
    "\n",
    "co_matrix = create_co_matrix(corpos_train, vectorizer.get_feature_names(), vectorizer.vocabulary_,\n",
    "                            window_size=3, use_weighting=True, verbose=True)\n",
    "\n",
    "co_matrix\n",
    "print(vectorizer.vocabulary_[\"allows\"])\n",
    "sms_ids=[]\n",
    "sms_ids.insert(1,vectorizer.vocabulary_[\"allows\"])\n",
    "for center_i, center_word_id in enumerate(sms_ids):\n",
    "    print(f\"center_i: {center_i}, center_word_id: {center_word_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立PPMI矩陣"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定義正向點間互資訊\n",
    "\n",
    "def ppmi(co_matrix: np.ndarray, eps: float=1e-8, verbose: bool=False):\n",
    "    M = np.zeros_like(co_matrix, dtype=np.float32)\n",
    "    N = np.sum(co_matrix)\n",
    "    S = np.sum(co_matrix, axis=0)\n",
    "    total = co_matrix.shape[0]*co_matrix.shape[1]\n",
    "\n",
    "    cnt = 0\n",
    "    \n",
    "    for i in range(co_matrix.shape[0]):\n",
    "        for j in range(co_matrix.shape[1]):\n",
    "            pmi = np.log2(co_matrix[i, j]*N / (S[i]*S[j] + eps))\n",
    "            M[i, j] = max(0, pmi)\n",
    "            \n",
    "            if verbose:\n",
    "                cnt += 1\n",
    "                if cnt % 10 == 0 or cnt == total:\n",
    "                    print(f\"{cnt}/{total} Done\")\n",
    "    \n",
    "    return M\n",
    "\n",
    "ppmi_matrix = ppmi(co_matrix, verbose=False)\n",
    "ppmi_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用SVD降維\n",
    "利用sklearn中的TruncatedSVD對co-occurrence matrix進行降維，並利用variance來找出最適合的維度\n",
    "[參考文獻](https://medium.com/swlh/truncated-singular-value-decomposition-svd-using-amazon-food-reviews-891d97af5d8d)\n",
    "\n",
    "(讀者可以嘗試使用SVD對PPMI進行降維)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Program to find the optimal number of components for Truncated SVD\n",
    "n_comp = range(10,150,10) # list containing different values of components\n",
    "variance_sum = [] # explained variance ratio for each component of Truncated SVD\n",
    "\n",
    "for dim in n_comp:\n",
    "    U, S, V = np.linalg.svd(ppmi_matrix)\n",
    "    variance_sum.append(V)\n",
    "\n",
    "plt.plot(n_comp, variance_sum)\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel(\"Explained Variance\")\n",
    "plt.title(\"Plot of Number of components v/s explained variance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose 140 as final dimension to reduce to \n",
    "# 利用上述找到的最適合dimension來對co-occurrence matrix進行降維\n",
    "U_reduce = U[:, 0:140]\n",
    "U_reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用KNN模型進行分類\n",
    "在進行分類之前，先利用簡單的詞向量平均來計算文本向量\n",
    "\n",
    "[參考文獻](https://medium.com/ai-academy-taiwan/nlp-%E4%B8%8D%E5%90%8C%E8%A9%9E%E5%90%91%E9%87%8F%E5%9C%A8%E6%96%87%E6%9C%AC%E5%88%86%E9%A1%9E%E4%B8%8A%E7%9A%84%E8%A1%A8%E7%8F%BE%E8%88%87%E5%AF%A6%E4%BD%9C-e72a2daecfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get doc vector via take mean of all word vectors inside the corresponding document\n",
    "\n",
    "def make_doc_vectors(corpus: List[str], word2idx: dict, vocab_list: List) -> List[np.ndarray]:\n",
    "    \n",
    "    # vectorizing data \n",
    "    # and make document vector by take mean to all word vecto\n",
    "    doc_vec = []\n",
    "    empty_doc_list = []\n",
    "    for i, sms_msg in enumerate(corpus):\n",
    "        sms_msg = [word2idx[word] for word in sms_msg.split() if word in vocab_list] #tokenize\n",
    "        if len(sms_msg) > 0:\n",
    "            sms_msg = np.array([re_co_matrix[ids] for ids in sms_msg]) #vectorize\n",
    "            doc_vec.append(sms_msg.mean(axis=0))\n",
    "        else:\n",
    "            empty_doc_list.append(i)\n",
    "            print(f\"document {i} doesn't contain word in vocab_list\")\n",
    "            print(corpus[i])\n",
    "            print(\"\\n\")\n",
    "        \n",
    "    return np.vstack(doc_vec), empty_doc_list\n",
    "\n",
    "word2idx = vectorizer.vocabulary_\n",
    "vocab_list = vectorizer.get_feature_names()\n",
    "\n",
    "doc_vec_train, missing_train_list = make_doc_vectors(x_train, word2idx, vocab_list)\n",
    "print(\"=\"*50)\n",
    "doc_vec_test, missing_test_list = make_doc_vectors(x_test, word2idx, vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# training\n",
    "y_train_filter = np.delete(np.array(y_train), missing_train_list)\n",
    "\n",
    "###<your code>###\n",
    "\n",
    "# testing\n",
    "y_test_filter = np.delete(np.array(y_test), missing_test_list)\n",
    "###<your code>###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"train acc: {np.sum(train_pred == y_train_filter) / len(y_train_filter)}\")\n",
    "print(f\"train acc: {np.sum(test_pred == y_test_filter) / len(y_test_filter)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
